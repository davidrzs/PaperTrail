{
  "users": [
    {
      "username": "demo",
      "email": "demo@papertrail.dev",
      "password": "demo123",
      "display_name": "Demo User",
      "bio": "Machine learning researcher interested in transformers and computer vision"
    }
  ],
  "papers": [
    {
      "title": "Attention Is All You Need",
      "authors": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia",
      "arxiv_id": "1706.03762",
      "doi": "10.48550/arXiv.1706.03762",
      "paper_url": "https://arxiv.org/abs/1706.03762",
      "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.",
      "summary": "Introduces the Transformer architecture, which uses self-attention mechanisms instead of recurrence or convolutions. This paper revolutionized NLP by showing that attention alone is sufficient for sequence transduction tasks, enabling massive parallelization and leading to models like BERT and GPT.",
      "tags": ["transformers", "attention", "nlp", "deep-learning"],
      "is_private": false,
      "date_read": "2024-01-15"
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "authors": "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian",
      "arxiv_id": "1512.03385",
      "doi": "10.1109/CVPR.2016.90",
      "paper_url": "https://arxiv.org/abs/1512.03385",
      "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.",
      "summary": "Introduces residual connections (skip connections) that enable training very deep neural networks (152+ layers) by allowing gradients to flow directly through the network. ResNet won ImageNet 2015 and became foundational for modern deep learning architectures.",
      "tags": ["computer-vision", "deep-learning", "resnet", "cnn"],
      "is_private": false,
      "date_read": "2024-01-20"
    },
    {
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",
      "arxiv_id": "1810.04805",
      "doi": "10.48550/arXiv.1810.04805",
      "paper_url": "https://arxiv.org/abs/1810.04805",
      "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks.",
      "summary": "Introduces bidirectional pre-training using masked language modeling and next sentence prediction. BERT showed that pre-training on large unlabeled corpora followed by task-specific fine-tuning could achieve state-of-the-art results across many NLP tasks.",
      "tags": ["transformers", "nlp", "pre-training", "bert"],
      "is_private": false,
      "date_read": "2024-02-01"
    },
    {
      "title": "Language Models are Few-Shot Learners",
      "authors": "Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
      "arxiv_id": "2005.14165",
      "doi": "10.48550/arXiv.2005.14165",
      "paper_url": "https://arxiv.org/abs/2005.14165",
      "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions.",
      "summary": "Introduces GPT-3, a 175B parameter language model that can perform tasks via in-context learning without fine-tuning. Demonstrated that scaling up model size enables few-shot and zero-shot learning capabilities, fundamentally changing how we think about using language models.",
      "tags": ["gpt", "few-shot-learning", "nlp", "large-language-models"],
      "is_private": false,
      "date_read": "2024-02-10"
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": "Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others",
      "arxiv_id": "2010.11929",
      "doi": "10.48550/arXiv.2010.11929",
      "paper_url": "https://arxiv.org/abs/2010.11929",
      "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks.",
      "summary": "Vision Transformer (ViT) shows that transformers can be directly applied to image patches without convolutions, achieving excellent results when pre-trained on large datasets. This paper bridges NLP and computer vision by demonstrating the general applicability of the transformer architecture.",
      "tags": ["transformers", "computer-vision", "vit", "attention"],
      "is_private": false,
      "date_read": "2024-02-15"
    },
    {
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
      "authors": "Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and others",
      "arxiv_id": "2005.11401",
      "doi": "10.48550/arXiv.2005.11401",
      "paper_url": "https://arxiv.org/abs/2005.11401",
      "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems.",
      "summary": "Introduces RAG, which combines neural retrieval with generation by retrieving relevant documents and conditioning generation on them. This approach enables language models to access external knowledge bases, improving factual accuracy and providing citations for generated content.",
      "tags": ["rag", "retrieval", "nlp", "knowledge-intensive"],
      "is_private": false,
      "date_read": "2024-03-01"
    },
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
      "arxiv_id": "2106.09685",
      "doi": "10.48550/arXiv.2106.09685",
      "paper_url": "https://arxiv.org/abs/2106.09685",
      "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive.",
      "summary": "Proposes low-rank adaptation (LoRA) which freezes pre-trained model weights and injects trainable rank decomposition matrices into each layer. This dramatically reduces the number of trainable parameters for fine-tuning (10,000x fewer) while maintaining or improving performance.",
      "tags": ["fine-tuning", "efficient-training", "nlp", "parameter-efficient"],
      "is_private": false,
      "date_read": "2024-03-10"
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny",
      "arxiv_id": "2201.11903",
      "doi": "10.48550/arXiv.2201.11903",
      "paper_url": "https://arxiv.org/abs/2201.11903",
      "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting.",
      "summary": "Shows that prompting large language models to generate intermediate reasoning steps (chain of thought) before answering dramatically improves performance on complex reasoning tasks like math word problems and commonsense reasoning. Simple but powerful technique that unlocks reasoning capabilities.",
      "tags": ["prompting", "reasoning", "nlp", "large-language-models"],
      "is_private": false,
      "date_read": "2024-03-20"
    }
  ]
}
